---
title: "Machine Learning Project"
author: "Andy Stogdale"
date: "23 May 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

The background of this project below is taken from the Coursera Machine Learning site:

"Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset)."

## Project Aim

My aim for this project is to create a machine learning model that will predict the exercise class of each observation as accurately as possible. This will done by formatting and cleansing the data appropriately and then exploring different machine learning algorithms until one is found that gives satisfactory prediction accuracy.

## Data

The training and testing data for this project come from the following links:

- https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
- https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har

For the sake of this project, I manually downloaded these CSV data files and put them directly into my working environment, so they could be loaded with the code:

```{r}
training <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testing <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
```

## Setup

To make the work reproducible, I included the library calls and set the random seed at the start of the script.

```{r}
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
set.seed(6991)
```

## Data Preparation

Firstly, I split up the training dataset into training and cross-validation datasets, with 60% being designated to training, and the other 40% used for cross-validation:

```{r}
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
trainData <- training[inTrain, ]
cvData <- training[-inTrain, ]
```

From this we can see the training dataset has 11776 rows, and the cross validation set has 7846 rows, both datasets having 160 columns.

```{r}
dim(trainData)
```

```{r}
dim(cvData)
```

Next, I had to cleanse the data. Just through having a basic look at the data, I could see the first six columns would not be relevant for prediction, so I removed these columns:

```{r}
trainData <- trainData[,7:160]
```

I then removed columns that had data with near zero variance. This was done by creating a vector of only the row names with not near zero variance, and using that to subset the training dataset.

```{r}
nzvData <- nearZeroVar(trainData, saveMetrics=TRUE)
goodRowNames <- rownames(nzvData[nzvData$nzv == FALSE,])
trainData <- trainData[,colnames(trainData) %in% goodRowNames]
```

Also, I removed all columns where at least half of the data was NA.

```{r}
trainData <- trainData[, colMeans(is.na(trainData)) <= 0.5]
```

All of this data cleansing was only performed on the training dataset, so I would have to remove the same columns from the cross validation and test sets.

```{r}
cvData <- cvData[,colnames(cvData) %in% colnames(trainData)]
testing <- testing[,colnames(testing) %in% colnames(trainData)]
```

After this data cleansing, there was only 54 columns remaining in the datasets.

```{r}
dim(trainData)
```

```{r}
dim(cvData)
```

## Machine Learning Algorithms For Prediction

### Decision Trees

The first algorithm I will try is the Decision Tree.

```{r}
rpartmodel <- train(classe ~ .,data=trainData, method = "rpart")
```

This diagram shows a visual representation of the decision tree:

```{r}
fancyRpartPlot(rpartmodel$finalModel)
```

To check the effectiveness of the model, I made predictions against the cross validation data set, and created a confusion matrix. As shown below, the accuracy of this was 57%, which isn't very good so I will explore other algorithms.

```{r}
rpartpredictions <- predict(rpartmodel$finalModel, newdata = cvData, type = "class")
cmRpart <- confusionMatrix(rpartpredictions, cvData$classe)
cmRpart$overall['Accuracy']
```

### Random Forests

Next, I tried using the random forests algorithm. I tuned the training control for this method, due to it taking a long time to fit this model.

```{r}
control <- trainControl(method="cv", number=3, allowParallel=T, verbose=T)
rfmodel <- train(classe ~ ., data=trainData, method = "rf", trControl=control)
```

I used the same methods as I did with decision trees to check the effectiveness of this model. This model yielded an accuracy of 99.64%, so I will use this as my final model.

```{r}
rfpredictions <- predict(rfmodel$finalModel, newdata = cvData, type = "class")
cmRforest <- confusionMatrix(rfpredictions, cvData$classe)
cmRforest$overall['Accuracy']
```

To make predictions for the testing set, to submit to coursera, I ran the following code. These outcomes proved to be all correct, thus further validating my choice of model.

```{r}
testingpredictions <- predict(rfmodel$finalModel, newdata = testing, type = "class")
testingpredictions
```



